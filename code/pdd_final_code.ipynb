{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22th trial / 240923\n",
    "- Chemberta-v1\n",
    "- new train unique / test canonical\n",
    "- loss function: **CUSTOM**\n",
    "- relu (X)\n",
    "- descriptor 10개\n",
    "- batch 64 / learning rate 1e-6\n",
    "- o1-preview\n",
    "- epoch 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, get_cosine_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 로드\n",
    "train_data = pd.read_csv('../data/train_new_canonical_unique.csv')\n",
    "test_data = pd.read_csv('../data/test_canonical.csv')\n",
    "\n",
    "# 다양한 분자 설명자를 계산하는 함수\n",
    "def compute_descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    descriptors = []\n",
    "    if mol is not None:\n",
    "        descriptors.extend([\n",
    "            Descriptors.MolWt(mol),\n",
    "            Descriptors.NumHAcceptors(mol),\n",
    "            Descriptors.NumHDonors(mol),\n",
    "            Descriptors.TPSA(mol),\n",
    "            Descriptors.MolLogP(mol),\n",
    "            Descriptors.NumRotatableBonds(mol),\n",
    "            Descriptors.NumAliphaticRings(mol),\n",
    "            Descriptors.NumAromaticRings(mol),\n",
    "            Descriptors.FractionCSP3(mol),\n",
    "            Descriptors.HeavyAtomCount(mol)\n",
    "            # 필요한 다른 설명자 추가 가능\n",
    "        ])\n",
    "    else:\n",
    "        descriptors = [0] * 10  # 설명자 수에 맞게 0으로 채움\n",
    "    return descriptors\n",
    "\n",
    "# 커스텀 데이터셋 클래스 정의\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, smiles, descriptors, labels=None, tokenizer=None, max_length=512):\n",
    "        self.smiles = smiles\n",
    "        self.descriptors = descriptors\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smile = self.smiles[idx]\n",
    "        inputs = self.tokenizer(smile, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "\n",
    "        descriptor = torch.tensor(self.descriptors[idx], dtype=torch.float)\n",
    "        inputs['descriptors'] = descriptor\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "            return inputs, label\n",
    "        return inputs\n",
    "\n",
    "# 커스텀 모델 정의 (ReLU 제거)\n",
    "class CustomChemBERTaModel(nn.Module):\n",
    "    def __init__(self, base_model, descriptor_size):\n",
    "        super(CustomChemBERTaModel, self).__init__()\n",
    "        self.base_model = base_model  # RobertaForSequenceClassification\n",
    "        \n",
    "        # 분자 설명자 처리용 FC 레이어\n",
    "        self.descriptor_fc = nn.Sequential(\n",
    "            nn.Linear(descriptor_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(128)\n",
    "        )\n",
    "        \n",
    "        # BERT 출력과 분자 설명자를 결합하는 FC 레이어\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(base_model.config.hidden_size + 128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        # 출력 활성화 함수 제거 (ReLU 제거)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, descriptors):\n",
    "        # BERT 모델의 출력 얻기\n",
    "        bert_outputs = self.base_model.roberta(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        bert_cls_output = bert_outputs[:, 0, :]  # [CLS] 토큰의 표현\n",
    "        \n",
    "        # 분자 설명자 처리\n",
    "        descriptor_outputs = self.descriptor_fc(descriptors)\n",
    "        \n",
    "        # BERT 출력과 분자 설명자 결합\n",
    "        combined = torch.cat((bert_cls_output, descriptor_outputs), dim=1)\n",
    "        output = self.fc(combined)\n",
    "        \n",
    "        return output  # ReLU 제거로 음수 값도 예측 가능\n",
    "\n",
    "# model list\n",
    "chembert = 'seyonec/ChemBERTa-zinc-base-v1'\n",
    "chembert77m = 'DeepChem/ChemBERTa-77M-MTR'\n",
    "\n",
    "# 데이터 전처리\n",
    "tokenizer = RobertaTokenizer.from_pretrained(chembert)\n",
    "train_smiles = train_data['can_smiles'].values\n",
    "train_labels = train_data['IC50_nM'].values  # IC50 값은 nM 단위로 제공\n",
    "test_smiles = test_data['can_smiles'].values\n",
    "\n",
    "# 레이블 로그 변환 (epsilon 추가)\n",
    "epsilon = 1e-6\n",
    "train_labels_log = np.log(train_labels + epsilon)\n",
    "train_labels_ic50 = np.exp(train_labels_log) - epsilon  # IC50_nM 값 복원\n",
    "\n",
    "# 전체 훈련 데이터에서 y_range_global 계산\n",
    "y_range_global = train_labels_ic50.max() - train_labels_ic50.min()\n",
    "\n",
    "# 분자 설명자 계산\n",
    "train_descriptors = [compute_descriptors(s) for s in train_smiles]\n",
    "test_descriptors = [compute_descriptors(s) for s in test_smiles]\n",
    "\n",
    "# 설명자에 대한 정규화/표준화\n",
    "scaler = StandardScaler()\n",
    "train_descriptors = scaler.fit_transform(train_descriptors)  # Train descriptors를 기준으로 fit\n",
    "test_descriptors = scaler.transform(test_descriptors)  # Test descriptors는 같은 스케일로 변환\n",
    "\n",
    "# 데이터셋 분할\n",
    "train_smiles, val_smiles, train_descriptors, val_descriptors, train_labels_log, val_labels_log = train_test_split(\n",
    "    train_smiles, train_descriptors, train_labels_log, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = SMILESDataset(train_smiles, train_descriptors, train_labels_log, tokenizer)\n",
    "val_dataset = SMILESDataset(val_smiles, val_descriptors, val_labels_log, tokenizer)\n",
    "test_dataset = SMILESDataset(test_smiles, test_descriptors, None, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=46, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=46)\n",
    "test_loader = DataLoader(test_dataset, batch_size=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 로드 및 초기화\n",
    "descriptor_size = 10  # 사용한 설명자의 수에 맞게 설정\n",
    "base_model = RobertaForSequenceClassification.from_pretrained(chembert, num_labels=1)\n",
    "model = CustomChemBERTaModel(base_model, descriptor_size)\n",
    "\n",
    "# 모델을 디바이스로 이동\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 옵티마이저 및 학습률 스케줄러 정의\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "num_epochs = 500\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer, num_warmup_steps=int(0.1 * num_training_steps), num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# 커스텀 손실 함수 수정\n",
    "def custom_loss(predictions_log_ic50, targets_log_ic50, y_range_global, k=10):\n",
    "    # IC50_nM 단위로 변환\n",
    "    predictions_ic50 = torch.exp(predictions_log_ic50)\n",
    "    targets_ic50 = torch.exp(targets_log_ic50)\n",
    "    \n",
    "    # A 계산 (NRMSE)\n",
    "    mse = torch.mean((predictions_ic50 - targets_ic50) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    y_range = torch.tensor(y_range_global, device=targets_ic50.device)\n",
    "    epsilon = 1e-6  # 분모가 0이 되는 것을 방지\n",
    "    nrmse = rmse / (y_range + epsilon)\n",
    "    A = nrmse\n",
    "    \n",
    "    # B 계산 (Correct Ratio)\n",
    "    # pIC50로 변환\n",
    "    predictions_pIC50 = 9 - torch.log10(predictions_ic50 + epsilon)\n",
    "    targets_pIC50 = 9 - torch.log10(targets_ic50 + epsilon)\n",
    "    errors = torch.abs(predictions_pIC50 - targets_pIC50)\n",
    "    B_values = torch.sigmoid(k * (0.5 - errors))\n",
    "    B = torch.mean(B_values)\n",
    "    \n",
    "    # 손실 함수 계산 (수정됨)\n",
    "    loss = A + (1 - B)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "# 검증 함수 정의\n",
    "def validate(model, val_loader, device, y_range_global):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels_log = batch\n",
    "            for key in inputs:\n",
    "                inputs[key] = inputs[key].to(device)\n",
    "            descriptors = inputs.pop('descriptors').to(device)\n",
    "            labels_log = labels_log.to(device)\n",
    "            outputs = model(**inputs, descriptors=descriptors).squeeze()\n",
    "            loss = custom_loss(outputs, labels_log, y_range_global)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "# 학습 루프 (조기 종료 포함)\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "patience = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        inputs, labels_log = batch\n",
    "        for key in inputs:\n",
    "            inputs[key] = inputs[key].to(device)\n",
    "        descriptors = inputs.pop('descriptors').to(device)\n",
    "        labels_log = labels_log.to(device)\n",
    "\n",
    "        outputs = model(**inputs, descriptors=descriptors).squeeze()\n",
    "        loss = custom_loss(outputs, labels_log, y_range_global)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 스케줄러 스텝\n",
    "    scheduler.step()\n",
    "\n",
    "    # 평균 학습 손실 계산\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # 검증 손실 계산\n",
    "    val_loss = validate(model, val_loader, device, y_range_global)\n",
    "\n",
    "    # 현재 에포크의 손실 출력\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.6f}, Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'chembert_22th.pt')  # 베스트 모델 저장\n",
    "        print(f\"Validation loss improved, saving model at epoch {epoch+1}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in validation loss, patience counter: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 테스트 데이터에 대한 예측\n",
    "model.load_state_dict(torch.load('chembert_22th.pt'))  # 베스트 모델 로드\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        inputs = batch\n",
    "        for key in inputs:\n",
    "            inputs[key] = inputs[key].to(device)\n",
    "        descriptors = inputs.pop('descriptors').to(device)\n",
    "        outputs = model(**inputs, descriptors=descriptors)\n",
    "        preds = outputs.squeeze().cpu().numpy()\n",
    "        predictions.extend(np.atleast_1d(preds))\n",
    "\n",
    "# 예측값을 지수화하여 IC50_nM 단위로 복원\n",
    "predictions_ic50 = np.exp(predictions) - epsilon  # epsilon을 빼줌\n",
    "\n",
    "# 예측 결과를 테스트 데이터에 저장\n",
    "test_data['IC50_nM'] = predictions_ic50\n",
    "test_data[['ID', 'IC50_nM']].to_csv('../data/results/chembert_22th.csv', index=False) #0.5694023363\t\n",
    "\n",
    "print(\"====================================Prediction Complete====================================\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
